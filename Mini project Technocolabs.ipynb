{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61d78bd7",
   "metadata": {},
   "source": [
    "Objective:- The aim of this project is to build a predictive model and find out the sales of each product at a particular store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7efdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import *\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c687d5b9",
   "metadata": {},
   "source": [
    "We will import common libraries which will be useful is our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6332302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data=pd.read_csv(\"D:\\Technocolabs\\Mini-project-01 June 2023\\Train.csv\")\n",
    "Test_data=pd.read_csv(\"D:\\Technocolabs\\Mini-project-01 June 2023\\Test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4888d7",
   "metadata": {},
   "source": [
    "Importing our train dataset into our enviournment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e50542",
   "metadata": {},
   "source": [
    "Now first we will work on our Train data, fit the model in this dataset and then finally we will test that model in our test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdb2dc5",
   "metadata": {},
   "source": [
    "But we need to preprocess both the datas in order to fit the model accurately "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d55bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c2037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e557d9",
   "metadata": {},
   "source": [
    "In both the datasets we can see the NaN values which we have to remove from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec51bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4accc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data[Train_data.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a70857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "Train_data.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860bc3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c46a688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339ee78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data=Train_data.dropna()\n",
    "Test_data=Test_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa66f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8820a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45583149",
   "metadata": {},
   "source": [
    "So now based on our objeective we would do the feature selection which means we will drop the unwanted columns which are unneccesary for our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b7a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data.drop(['Item_Identifier','Outlet_Identifier'],axis=1,inplace=True)\n",
    "Test_data.drop(['Item_Identifier','Outlet_Identifier'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030542e3",
   "metadata": {},
   "source": [
    "We have removed the columns \"Item_Identifier\" and \"Outlet_Identifier\" from our data set as it is just a labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdcf7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Data\n",
    "Train_data=Train_data.replace(\"LF\", \"Low Fat\")\n",
    "Train_data=Train_data.replace(\"reg\", \"Regular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770dd1db",
   "metadata": {},
   "source": [
    "Now we will label the data columns like Item fat content item tye etc. In short we will label every catogerical data into numeric values like 0,1,2,3.... using one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fe1025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create object for one-hot encoding\n",
    "encoder=ce.OneHotEncoder(cols='Item_Fat_Content',handle_unknown='return_nan',return_df=True,use_cat_names=True)\n",
    "Train_data_encoded = encoder.fit_transform(Train_data)\n",
    "encoder=ce.OneHotEncoder(cols='Item_Type',handle_unknown='return_nan',return_df=True,use_cat_names=True)\n",
    "Train_data_encoded = encoder.fit_transform(Train_data_encoded)\n",
    "encoder=ce.OneHotEncoder(cols='Outlet_Size',handle_unknown='return_nan',return_df=True,use_cat_names=True)\n",
    "Train_data_encoded = encoder.fit_transform(Train_data_encoded)\n",
    "encoder=ce.OneHotEncoder(cols='Outlet_Location_Type',handle_unknown='return_nan',return_df=True,use_cat_names=True)\n",
    "Train_data_encoded = encoder.fit_transform(Train_data_encoded)\n",
    "encoder=ce.OneHotEncoder(cols='Outlet_Type',handle_unknown='return_nan',return_df=True,use_cat_names=True)\n",
    "Train_data_encoded = encoder.fit_transform(Train_data_encoded)\n",
    "Train_data_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fa65aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data = Train_data_encoded.apply(LabelEncoder().fit_transform)\n",
    "Test_data = Test_data.apply(LabelEncoder().fit_transform)\n",
    "#df_test = df_test.apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cce7541",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b732e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f0d051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4ddcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y3= Train_data[[\"Item_Outlet_Sales\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a1d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = Train_data\n",
    "x3=x3.drop([\"Item_Outlet_Sales\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e507a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7011ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(x3, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8c88a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pre_lr=lr.predict(x3)\n",
    "Y_pre_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1d9017",
   "metadata": {},
   "outputs": [],
   "source": [
    "x3.shape\n",
    "#y3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65cd59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_lr=lr.predict(x3)\n",
    "print(r2_score(y3, Y_pred_lr))\n",
    "print(mean_absolute_error(y3, Y_pred_lr))\n",
    "print(np.sqrt(mean_squared_error(y3, Y_pred_lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d385e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ffd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd47de4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf.fit(x3,y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e559c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8806c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_rf=rf.predict(x3)\n",
    "print(r2_score(y3, Y_pred_rf))\n",
    "print(mean_absolute_error(y3, Y_pred_rf))\n",
    "print(np.sqrt(mean_squared_error(y3, Y_pred_rf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd585ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_rf=rf.predict(x3)\n",
    "print(Y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c959c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23430ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif=pd.DataFrame()\n",
    "vif['vif']=[variance_inflation_factor(x3.values,i) for i in range (x3.shape[1])]\n",
    "vif['features']=x3.columns\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37307f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacf6a14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941044b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84fd981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b5d7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "dataplot = sns.heatmap(Train_data.corr(), cmap=\"YlGnBu\", annot=False)\n",
    "# displaying heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0a4419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82e8a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=Train_data.drop('Item_Outlet_Sales',axis=1)\n",
    "Y=Train_data['Item_Outlet_Sales']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, random_state=101, test_size=0.2)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a814c16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_std= sc.fit_transform(X_train)\n",
    "X_test_std= sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e57a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57f8b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train_std, Y_train)\n",
    "Y_pre_lr=lr.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e244f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(Y_test, Y_pre_lr))\n",
    "print(mean_absolute_error(Y_test, Y_pre_lr))\n",
    "print(np.sqrt(mean_squared_error(Y_test, Y_pre_lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d48f16d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0975b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032f2cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b52bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into two subsets: one with missing values and one without missing values in Item_Weight column\n",
    "missing_Train_data = Train_data[Train_data['Item_Weight'].isnull()]\n",
    "not_missing_Train_data = Train_data[~Train_data['Item_Weight'].isnull()]\n",
    "missing_Train_data = missing_Train_data.drop(['Outlet_Size'], axis=1)\n",
    "not_missing_Train_data = not_missing_Train_data.drop(['Outlet_Size'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12e7837",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "not_missing_Train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ffb7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = not_missing_Train_data[['Item_Fat_Content']]\n",
    "y_train = not_missing_Train_data['Item_Weight']\n",
    "X_test = missing_Train_data[['Item_Fat_Content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df996796",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.replace(\"Low Fat\",0)\n",
    "X_train=X_train.replace(\"Regular\",1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94089b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test.replace(\"Low Fat\",0)\n",
    "X_test=X_test.replace(\"Regular\",1)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20886056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692be5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the missing values\n",
    "predicted_values = model.predict(X_test)\n",
    "\n",
    "# Fill the missing values in the original dataset with the predicted values\n",
    "data.loc[Train_data['Item_Weight'].isnull(), 'Item_Weight'] = predicted_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
